{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Set up Spark\n",
    "spark = SparkSession.builder.appName('nb').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Helper method to read heartbeat dataset\n",
    "def readHeartDataset():\n",
    "    # Load training data\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"s3://big-data-spark-project-24/statlog_heart.csv\")\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['patient_id']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['age', 'sex', 'chest pain type', 'resting blood pressure', 'serum cholestoral', 'fasting blood sugar', 'resting electrocardiographic results', 'maximum heart rate achieved', 'exercise induced angina', 'oldpeak', 'the slope of the peak exercise ST segment', 'number of major vessels', 'thal']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'class'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read airline satisfaction dataset\n",
    "def readAirlineSatisfactionDataset():\n",
    "    # Load training data\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"s3://big-data-spark-project-24/airline.csv\")\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    gender_indexer = StringIndexer(inputCol = 'Gender', outputCol = 'Gender Index')\n",
    "    customer_type_indexer = StringIndexer(inputCol = 'Customer Type', outputCol = 'Customer Type Index')\n",
    "    type_of_travel_indexer = StringIndexer(inputCol = 'Type of Travel', outputCol = 'Type of Travel Index')\n",
    "    class_indexer = StringIndexer(inputCol = 'Class', outputCol = 'Class Index')\n",
    "    satisfaction_indexer = StringIndexer(inputCol = 'satisfaction', outputCol = 'Satisfaction Index')\n",
    "\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = customer_type_indexer.fit(df).transform(df)\n",
    "    df = type_of_travel_indexer.fit(df).transform(df)\n",
    "    df = class_indexer.fit(df).transform(df)\n",
    "    df = satisfaction_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['id', 'Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', \n",
    "                    'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', \n",
    "                    'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', \n",
    "                    'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes', \n",
    "                    'Gender Index', 'Customer Type Index', 'Type of Travel Index', 'Class Index']\n",
    "    \n",
    "    # Set label column\n",
    "    label_col = 'Satisfaction Index'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read credit card fraud dataset\n",
    "def readFraudDataset():\n",
    "    # Load training data\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"s3://big-data-spark-project-24/fraudTrain.csv\")\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0', 'trans_date_trans_time', 'cc_num', 'merchant', 'first', 'last', 'street', \n",
    "                    'city', 'state', 'zip', 'dob', 'trans_num', 'unix_time']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    merchant_indexer = StringIndexer(inputCol='category', outputCol='categoryIndex')\n",
    "    gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "    job_indexer = StringIndexer(inputCol='job', outputCol='jobIndex')\n",
    "\n",
    "    df = merchant_indexer.fit(df).transform(df)\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = job_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['gender', 'category', 'job']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    df = df.withColumn('lat', col('lat') + 200)\n",
    "    df = df.withColumn('long', col('long') + 200)\n",
    "    df = df.withColumn('merch_lat', col('merch_lat') + 200)\n",
    "    df = df.withColumn('merch_long', col('merch_long') + 200)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'categoryIndex', 'genderIndex', 'jobIndex']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'is_fraud'\n",
    "    \n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read particle dataset\n",
    "def readParticleDataset():\n",
    "    # Load training data\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"s3://big-data-spark-project-24/SUSY.csv\")\n",
    "    column_names = ['class', 'lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi', 'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R', 'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos_theta_r1']\n",
    "    df = df.toDF(*column_names)\n",
    "\n",
    "    df = df.withColumn('lepton_1_eta', col('lepton_1_eta') + 3)\n",
    "    df = df.withColumn('lepton_1_phi', col('lepton_1_phi') + 2)\n",
    "    df = df.withColumn('lepton_2_eta', col('lepton_2_eta') + 3)\n",
    "    df = df.withColumn('lepton_2_phi', col('lepton_2_phi') + 2)\n",
    "    df = df.withColumn('missing_energy_phi', col('missing_energy_phi') + 2)\n",
    "    df = df.withColumn('axial_MET', col('axial_MET') + 17)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi', 'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R', 'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos_theta_r1']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'class'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Read dataset (change to test different datasets)\n",
    "(df, feature_cols, label_col) = readHeartDataset()\n",
    "df = df.select([col(column).cast(\"float\").alias(column) for column in feature_cols + [label_col]])\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid=\"skip\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = df.randomSplit([0.7, 0.3], seed=1)\n",
    "training_data.show(n=3)\n",
    "\n",
    "# Initialize Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial', labelCol=label_col)\n",
    "pipeline = Pipeline(stages=[assembler, nb])\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(n=5)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print the time taken\n",
    "print(f'Time taken: {round(end - start, 2)}s')\n",
    "\n",
    "# Evaluate the model's performance and print accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol=label_col)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Accuracy: {round(accuracy * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
