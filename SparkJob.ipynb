{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+\n",
      "| age|sex|chest-pain|rest-bp|serum-chol|fasting-blood-sugar|electrocardiographic|max-heart-rate|angina|oldpeak|slope|major-vessels|thal|heart-disease|\n",
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+\n",
      "|29.0|1.0|       2.0|  130.0|     204.0|                0.0|                 2.0|         202.0|   0.0|    0.0|  1.0|          0.0| 3.0|          0.0|\n",
      "|34.0|0.0|       2.0|  118.0|     210.0|                0.0|                 0.0|         192.0|   0.0|    0.7|  1.0|          0.0| 3.0|          0.0|\n",
      "|34.0|1.0|       1.0|  118.0|     182.0|                0.0|                 2.0|         174.0|   0.0|    0.0|  1.0|          0.0| 3.0|          0.0|\n",
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+--------------------+--------------------+--------------------+----------+\n",
      "| age|sex|chest-pain|rest-bp|serum-chol|fasting-blood-sugar|electrocardiographic|max-heart-rate|angina|oldpeak|slope|major-vessels|thal|heart-disease|            features|       rawPrediction|         probability|prediction|\n",
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|35.0|1.0|       4.0|  120.0|     198.0|                0.0|                 0.0|         130.0|   1.0|    1.6|  2.0|          0.0| 7.0|          1.0|[35.0,1.0,4.0,120...|[-712.17398655155...|[0.42266492086692...|       1.0|\n",
      "|35.0|1.0|       4.0|  126.0|     282.0|                0.0|                 2.0|         156.0|   1.0|    0.0|  1.0|          0.0| 7.0|          1.0|[35.0,1.0,4.0,126...|[-826.74695083871...|[0.75031652006023...|       0.0|\n",
      "|40.0|1.0|       4.0|  110.0|     167.0|                0.0|                 2.0|         114.0|   1.0|    2.0|  2.0|          0.0| 7.0|          1.0|[40.0,1.0,4.0,110...|[-675.81960026831...|[0.08278582789785...|       1.0|\n",
      "|40.0|1.0|       4.0|  152.0|     223.0|                0.0|                 0.0|         181.0|   0.0|    0.0|  1.0|          0.0| 7.0|          1.0|[40.0,1.0,4.0,152...|[-837.74960578195...|[0.99967288973976...|       0.0|\n",
      "|41.0|0.0|       2.0|  105.0|     198.0|                0.0|                 0.0|         168.0|   0.0|    0.0|  1.0|          1.0| 3.0|          0.0|[41.0,0.0,2.0,105...|[-698.88914003363...|[0.99985906363829...|       0.0|\n",
      "+----+---+----------+-------+----------+-------------------+--------------------+--------------+------+-------+-----+-------------+----+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time taken: 0.2346179485321045\n",
      "Accuracy: 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('nb').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Helper method to read heartbeat dataset\n",
    "def readHeartDataset():\n",
    "    file_location = 'heart.dat'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, sep=' ', schema='age float, sex float, `chest-pain` float, `rest-bp` float, `serum-chol` float, `fasting-blood-sugar` float, electrocardiographic float, `max-heart-rate` float, angina float, oldpeak float, slope float, `major-vessels` float, thal float, `heart-disease` float')\n",
    "\n",
    "    # Normalize heart-disease column\n",
    "    df = df.withColumn('heart-disease', df['heart-disease'] - 1)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['age', 'sex', 'chest-pain', 'rest-bp', 'serum-chol', 'fasting-blood-sugar', 'electrocardiographic', 'max-heart-rate', 'angina', 'oldpeak', 'slope', 'major-vessels', 'thal']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'heart-disease'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read airline satisfaction dataset\n",
    "def readAirlineSatisfactionDataset():\n",
    "    file_location = 'airline.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    gender_indexer = StringIndexer(inputCol = 'Gender', outputCol = 'Gender Index')\n",
    "    customer_type_indexer = StringIndexer(inputCol = 'Customer Type', outputCol = 'Customer Type Index')\n",
    "    type_of_travel_indexer = StringIndexer(inputCol = 'Type of Travel', outputCol = 'Type of Travel Index')\n",
    "    class_indexer = StringIndexer(inputCol = 'Class', outputCol = 'Class Index')\n",
    "    satisfaction_indexer = StringIndexer(inputCol = 'satisfaction', outputCol = 'Satisfaction Index')\n",
    "\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = customer_type_indexer.fit(df).transform(df)\n",
    "    df = type_of_travel_indexer.fit(df).transform(df)\n",
    "    df = class_indexer.fit(df).transform(df)\n",
    "    df = satisfaction_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['id', 'Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', \n",
    "                    'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', \n",
    "                    'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', \n",
    "                    'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes', \n",
    "                    'Gender Index', 'Customer Type Index', 'Type of Travel Index', 'Class Index']\n",
    "    \n",
    "    # Set label column\n",
    "    label_col = 'Satisfaction Index'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read credit card fraud dataset\n",
    "def readFraudDataset():\n",
    "    file_location = 'fraudTrain.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0', 'trans_date_trans_time', 'cc_num', 'merchant', 'first', 'last', 'street', \n",
    "                    'city', 'state', 'zip', 'dob', 'trans_num', 'unix_time']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    merchant_indexer = StringIndexer(inputCol='category', outputCol='categoryIndex')\n",
    "    gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "    job_indexer = StringIndexer(inputCol='job', outputCol='jobIndex')\n",
    "\n",
    "    df = merchant_indexer.fit(df).transform(df)\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = job_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['gender', 'category', 'job']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    df = df.withColumn('lat', col('lat') + 200)\n",
    "    df = df.withColumn('long', col('long') + 200)\n",
    "    df = df.withColumn('merch_lat', col('merch_lat') + 200)\n",
    "    df = df.withColumn('merch_long', col('merch_long') + 200)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'categoryIndex', 'genderIndex', 'jobIndex']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'is_fraud'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read particle dataset\n",
    "def readParticleDataset():\n",
    "    file_location = 'SUSY.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, inferSchema=True)\n",
    "    column_names = ['class', 'lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi', 'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R', 'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos_theta_r1']\n",
    "    df = df.toDF(*column_names)\n",
    "\n",
    "    df = df.withColumn('lepton_1_eta', col('lepton_1_eta') + 3)\n",
    "    df = df.withColumn('lepton_1_phi', col('lepton_1_phi') + 2)\n",
    "    df = df.withColumn('lepton_2_eta', col('lepton_2_eta') + 3)\n",
    "    df = df.withColumn('lepton_2_phi', col('lepton_2_phi') + 2)\n",
    "    df = df.withColumn('missing_energy_phi', col('missing_energy_phi') + 2)\n",
    "    df = df.withColumn('axial_MET', col('axial_MET') + 17)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi', 'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R', 'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos_theta_r1']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'class'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Read dataset (change to test different datasets)\n",
    "(df, feature_cols, label_col) = readParticleDataset()\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid=\"skip\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = df.randomSplit([0.7, 0.3], seed=1)\n",
    "training_data.show(n = 3)\n",
    "\n",
    "# Initialize Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial', labelCol=label_col)\n",
    "pipeline = Pipeline(stages=[assembler, nb])\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(n=5)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print the time taken\n",
    "print(f'Time taken: {end - start}')\n",
    "\n",
    "# Evaluate the model's performance and print accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol=label_col)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
