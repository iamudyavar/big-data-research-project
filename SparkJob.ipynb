{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/28 12:17:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+--------------------+\n",
      "|class|        lepton_1_pT|      lepton_1_eta|      lepton_1_phi|        lepton_2_pT|      lepton_2_eta|      lepton_2_phi|missing_energy_magnitude|missing_energy_phi|           MET_rel|         axial_MET|                M_R|            M_TR_2|                 R|               MT2|                S_R|         M_Delta_R|          dPhi_r_b|        cos_theta_r1|\n",
      "+-----+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+--------------------+\n",
      "|  0.0|0.25550660490989685|3.5993970036506653|1.9439021460711956|0.42958030104637146|2.9827059749513865| 3.170736074447632|     0.45079344511032104|0.8121740818023682|0.6766965389251709|16.863651394844055| 0.2854113280773163|0.4587327837944031|1.4262809753417969|0.7935577034950256| 0.3206666111946106|0.6220175623893738|1.5735169649124146|0.012640399858355522|\n",
      "|  0.0| 0.2570890486240387| 1.813449740409851|0.7848457098007202|0.43229755759239197| 2.008798897266388|1.6288348138332367|      0.7977002859115601|3.2958754301071167|1.1974447965621948|16.669303238391876|0.27028533816337585|0.6409867405891418|2.1044764518737793|1.2521220445632935|0.35071614384651184| 0.953214168548584|1.2968096733093262| 0.36174699664115906|\n",
      "|  0.0| 0.2590164840221405| 2.748418152332306|1.0272763967514038|0.42893311381340027|1.7808500528335571|2.6945905089378357|      1.1228281259536743|1.4043113589286804|1.0466327667236328|18.174092531204224| 0.3171117901802063|0.5900524258613586|1.6511784791946411|               0.0|0.40333661437034607| 0.626440703868866| 1.570011019706726|   0.454039990901947|\n",
      "+-----+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|class|        lepton_1_pT|      lepton_1_eta|      lepton_1_phi|       lepton_2_pT|      lepton_2_eta|      lepton_2_phi|missing_energy_magnitude|missing_energy_phi|            MET_rel|         axial_MET|                M_R|             M_TR_2|                 R|               MT2|                S_R|           M_Delta_R|          dPhi_r_b|       cos_theta_r1|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|0.25550660490989685|1.0978240966796875|1.5786563456058502|0.4293231666088104|1.0715974569320679|3.4885473251342773|      0.5428950190544128|3.4692662954330444|0.03743349760770798| 17.37472575902939| 0.2673070430755615| 0.4153663218021393|1.3789136409759521| 0.040892593562603|0.29987677931785583|0.048146072775125504|1.4410099983215332| 0.4685690104961395|[0.25550660490989...|[-67.560693134094...|[0.87201076044129...|       0.0|\n",
      "|  0.0| 0.2556491792201996| 1.498552918434143|3.4123986959457397| 0.430289626121521| 3.079480156302452|  1.31378835439682|      0.5655290484428406|  3.53333842754364|0.17549626529216766| 17.60762745141983| 0.3947335481643677| 0.3907553255558014|0.8784490823745728|               0.0| 0.4101247191429138| 0.34939637780189514|1.3432838916778564| 0.2223850041627884|[0.25564917922019...|[-71.981789414928...|[0.88667109434147...|       0.0|\n",
      "|  0.0|  0.255950927734375| 2.660633534193039|0.4471367597579956|0.4293724298477173| 2.986038541421294| 2.799129009246826|      0.2764769196510315|0.3013620376586914|0.11383818089962006|17.523153960704803|0.27232593297958374| 0.2327246218919754|0.7583499550819397|               0.0|0.23977436125278473| 0.06543567031621933|0.8804314136505127|0.21523800492286682|[0.25595092773437...|[-54.303015929512...|[0.88753843681114...|       0.0|\n",
      "|  0.0| 0.2569057047367096| 4.467935085296631|0.8261232376098633|0.4290081262588501| 4.132717847824097|3.3659499883651733|      0.5569479465484619| 2.159158170223236| 0.8360475301742554|16.759435296058655|0.27236589789390564| 0.5478126406669617|1.7848206758499146| 1.079636812210083|0.32131868600845337|  0.8181794881820679|1.5127960443496704|0.05497860163450241|[0.25690570473670...|[-87.122005015361...|[0.86281514591269...|       0.0|\n",
      "|  0.0|  0.258768230676651| 3.171378120779991|0.9211142063140869|0.4325525760650635| 2.470111310482025|3.5442644357681274|      0.4102104604244232|2.3999979197978973| 0.6157764792442322|16.921149380505085| 0.2944241464138031|0.47590479254722595|1.4343775510787964|0.9540677666664124|0.30900248885154724|  0.7119353413581848|1.1176156997680664|0.09721619635820389|[0.25876823067665...|[-76.163784933024...|[0.85886896392982...|       0.0|\n",
      "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time taken: 10.03225588798523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>            (14 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7176366380377762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('nb').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Helper method to read heartbeat dataset\n",
    "def readHeartDataset():\n",
    "    file_location = 'heart.dat'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, sep=' ', schema='age float, sex float, `chest-pain` float, `rest-bp` float, `serum-chol` float, `fasting-blood-sugar` float, electrocardiographic float, `max-heart-rate` float, angina float, oldpeak float, slope float, `major-vessels` float, thal float, `heart-disease` float')\n",
    "\n",
    "    # Normalize heart-disease column\n",
    "    df = df.withColumn('heart-disease', df['heart-disease'] - 1)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['age', 'sex', 'chest-pain', 'rest-bp', 'serum-chol', 'fasting-blood-sugar', 'electrocardiographic', 'max-heart-rate', 'angina', 'oldpeak', 'slope', 'major-vessels', 'thal']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'heart-disease'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read airline satisfaction dataset\n",
    "def readAirlineSatisfactionDataset():\n",
    "    file_location = 'airline.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    gender_indexer = StringIndexer(inputCol = 'Gender', outputCol = 'Gender Index')\n",
    "    customer_type_indexer = StringIndexer(inputCol = 'Customer Type', outputCol = 'Customer Type Index')\n",
    "    type_of_travel_indexer = StringIndexer(inputCol = 'Type of Travel', outputCol = 'Type of Travel Index')\n",
    "    class_indexer = StringIndexer(inputCol = 'Class', outputCol = 'Class Index')\n",
    "    satisfaction_indexer = StringIndexer(inputCol = 'satisfaction', outputCol = 'Satisfaction Index')\n",
    "\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = customer_type_indexer.fit(df).transform(df)\n",
    "    df = type_of_travel_indexer.fit(df).transform(df)\n",
    "    df = class_indexer.fit(df).transform(df)\n",
    "    df = satisfaction_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    df.show(n=5)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['id', 'Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', \n",
    "                    'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', \n",
    "                    'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', \n",
    "                    'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes', \n",
    "                    'Gender Index', 'Customer Type Index', 'Type of Travel Index', 'Class Index']\n",
    "    \n",
    "    # Set label column\n",
    "    label_col = 'Satisfaction Index'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read credit card fraud dataset\n",
    "def readFraudDataset():\n",
    "    file_location = 'fraudTrain.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "\n",
    "    # Preprocess data\n",
    "    columns_to_drop = ['_c0', 'trans_date_trans_time', 'cc_num', 'merchant', 'first', 'last', 'street', \n",
    "                    'city', 'state', 'zip', 'dob', 'trans_num', 'unix_time']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    merchant_indexer = StringIndexer(inputCol='category', outputCol='categoryIndex')\n",
    "    gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "    job_indexer = StringIndexer(inputCol='job', outputCol='jobIndex')\n",
    "\n",
    "    df = merchant_indexer.fit(df).transform(df)\n",
    "    df = gender_indexer.fit(df).transform(df)\n",
    "    df = job_indexer.fit(df).transform(df)\n",
    "\n",
    "    columns_to_drop = ['gender', 'category', 'job']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    df = df.withColumn('lat', col('lat') + 200)\n",
    "    df = df.withColumn('long', col('long') + 200)\n",
    "    df = df.withColumn('merch_lat', col('merch_lat') + 200)\n",
    "    df = df.withColumn('merch_long', col('merch_long') + 200)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'categoryIndex', 'genderIndex', 'jobIndex']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'is_fraud'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Helper method to read particle dataset\n",
    "def readParticleDataset():\n",
    "    file_location = 'SUSY.csv'\n",
    "\n",
    "    # Load training data\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "\n",
    "    df = df.withColumn('lepton_1_eta', col('lepton_1_eta') + 3)\n",
    "    df = df.withColumn('lepton_1_phi', col('lepton_1_phi') + 2)\n",
    "    df = df.withColumn('lepton_2_eta', col('lepton_2_eta') + 3)\n",
    "    df = df.withColumn('lepton_2_phi', col('lepton_2_phi') + 2)\n",
    "    df = df.withColumn('missing_energy_phi', col('missing_energy_phi') + 2)\n",
    "    df = df.withColumn('axial_MET', col('axial_MET') + 17)\n",
    "\n",
    "    # Set feature columns\n",
    "    feature_cols = ['lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi', 'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude', 'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R', 'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos_theta_r1']\n",
    "\n",
    "    # Set label column\n",
    "    label_col = 'class'\n",
    "\n",
    "    return (df, feature_cols, label_col)\n",
    "\n",
    "# Read dataset (change to test different datasets)\n",
    "(df, feature_cols, label_col) = readParticleDataset()\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid=\"skip\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = df.randomSplit([0.7, 0.3])\n",
    "training_data.show(n = 3)\n",
    "\n",
    "# Initialize Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial', labelCol=label_col)\n",
    "pipeline = Pipeline(stages=[assembler, nb])\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(n=5)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print the time taken\n",
    "print(f'Time taken: {end - start}')\n",
    "\n",
    "# Evaluate the model's performance and print accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol=label_col)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
